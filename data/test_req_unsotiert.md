# State-of-the-Art Lösungen für vollautomatisiertes Requirements-Mining und -Evaluieren aus Textdokumenten (2025)

## Überblick

Diese Zusammenstellung bietet einen umfassenden Überblick über Best Practices und neueste  **State-of-the-Art** -Verfahren (Stand 2025) für das vollautomatisierte Requirements-Engineering aus Textdokumenten. Der Fokus liegt auf Markdown- und Freitext-Dokumenten. Alle geforderten Schritte – von der Datei- und Struktur-Erkennung über AST-Parsing, Segmentierung, Offset-Verfolgung, Kontextverknüpfung, Deduplikation bis zur Ausgabe – werden adressiert. Genannte Lösungen beziehen sich auf aktuelle Open-Source-Projekte, Frameworks und wissenschaftliche Publikationen aus dem Jahr 2025.

## 1. Vollautomatisierte Erkennung von Dateityp und Strukturindikatoren

### State-of-the-Art 2025

* **Magika** (Google, Open Source): Ein KI-gestützter Content-Type-Classifier, der 2025 auf der ICSE vorgestellt wurde. Magika nutzt ein kompaktes Deep-Learning-Modell (nur wenige MB), das über 200 Dateitypen – inklusive Markdown und Klartext – mit ~99% Genauigkeit erkennen kann[github.com](https://github.com/google/magika#:~:text=Magika%20is%20a%20novel%20AI,accuracy%20on%20our%20test%20set)[github.com](https://github.com/google/magika#:~:text=For%20more%20context%20you%20can,ICSE%29%202025). Durch Analyse von Datei-Bytes und Inhalten identifiziert Magika den Dateityp extrem schnell (ca. 5 ms pro Datei) und liefert damit Hinweise, ob es sich z.B. um reinen Text oder Markdown mit Formatierung handelt[github.com](https://github.com/google/magika#:~:text=Magika%20is%20a%20novel%20AI,accuracy%20on%20our%20test%20set)[github.com](https://github.com/google/magika#:~:text=Magika%20is%20used%20at%20scale,ch%20%28example). Strukturmerkmale wie typische Markdown-Syntax (z.B. `# ` für Überschriften oder `- ` für Listen) werden implizit durch das Modell erfasst, was die Weichenstellung für Folgeprozesse erleichtert.
* **Docling v2** (IBM, Open Source): Ein umfassendes Toolkit für Document Understanding, das verschiedene Formate (PDF, DOCX, HTML, Markdown, Bilder u.a.) in eine einheitliche, reich strukturierte Repräsentation überführt[arxiv.org](https://arxiv.org/abs/2501.17887#:~:text=open,source%20frameworks%20%28e.g.%2C%20LangChain). Docling nutzt spezialisierte KI-Modelle für Layout-Analyse (DocLayNet) und Tabellenerkennung (TableFormer)[arxiv.org](https://arxiv.org/abs/2501.17887#:~:text=open,source%20frameworks%20%28e.g.%2C%20LangChain). Bereits beim Parsen werden Dokument-Elemente wie Überschriften, Absätze, Listen und Tabellen mitsamt Hierarchie erkannt[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=)[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=Layout%20information%20%28i,for%20all%20items%2C%20if%20available). Große Dokumente können in einem Streaming-Verfahren verarbeitet werden, um früh Meta-Informationen über die Struktur (z.B. Überschriftenebenen, Tabellen) bereitzustellen. Intern entscheidet Docling anhand des Dateiformats zwischen unterschiedlichen Parser-Backends und Pipelines[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=Conversion%20arxiv,to%20a%20fitting%20processing%20pipeline) – für Markup-Formate (Markdown, HTML etc.) nutzt es die vorhandene semantische Struktur, während für PDFs/Bilder OCR und Layout-Modelle zum Einsatz kommen[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=Document%20formats%20can%20be%20broadly,categorized%20into%20two%20types)[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=2).
* **Surya (OCR Toolkit)** : Ein Open-Source-Werkzeug, das ursprünglich für OCR und Layout-Analyse entwickelt wurde. Surya erkennt in gescannten oder PDF-Dokumenten Texte, Bilder, **Tabellen** und sogar Kopfzeilen und ordnet die erkannten Elemente in der richtigen Leserichtung an[github.com](https://github.com/datalab-to/surya#:~:text=Surya%20is%20a%20document%20OCR,toolkit%20that%20does)[github.com](https://github.com/datalab-to/surya#:~:text=,LaTeX%20OCR). Insbesondere für Tabellen ist Surya führend: Es detektiert Tabellenbereiche und identifiziert Zeilen- und Spaltenstrukturen zuverlässig[github.com](https://github.com/datalab-to/surya#:~:text=,LaTeX%20OCR)[github.com](https://github.com/datalab-to/surya#:~:text=Surya%20is%20a%20document%20OCR,toolkit%20that%20does). In Kombination mit einem Textparser kann Surya so eingesetzt werden, um bereits beim Pre-Processing Tabellen von Fließtext zu unterscheiden und unterschiedliche Folge-Strategien (Tabellen-Extraktion vs. normaler Textparser) einzuleiten.

**Best Practice 2025:**

Eine robuste Pipeline startet mit **Magika** zur Dateityp-Erkennung (z.B. Markdown vs. reiner Text vs. PDF) und leitet dann an spezialisierte Parser weiter. **Docling** kann für formattierte Dokumente die Dokument-Struktur extrahieren (Überschriftenhierarchie, Listen, Tabellen) und ein einheitliches internes Modell erzeugen[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=Docling%20v2%20introduces%20a%20unified,common%20document%20features%2C%20such%20as)[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=Besides%20specifying%20the%20data%20model%2C,retain%20all%20available%20meta%20information). Für PDFs oder Bilder ergänzt **Surya** die Erkennung, indem es Tabellen und Layout auf visueller Ebene identifiziert. Durch diese Kombination werden Format und Struktur eines unbekannten Dokuments vollautomatisch erkannt, ohne dass Vorwissen über die Quelle (Dateiendung o.Ä.) nötig ist.

## 2. Markdown-Parsing zu AST (Abstract Syntax Tree) mit Offsets, Ausschluss von Codeblöcken/Blockquotes, Extraktion von Tabellen-Headern (inkl. Aliasse)

### Frameworks und Methoden

* **Remark/MDAST** : Das **Markdown AST** („mdast“) ist der De-facto-Standard für die Repräsentation von Markdown-Strukturen. Parser wie *remark-parse* (JavaScript, Teil des unified-Frameworks) erzeugen einen Syntaxbaum, der alle Elemente (Überschriften, Absätze, Listen, Codeblöcke, Zitate, Tabellen etc.) als Knoten enthält. Entscheidender Vorteil: Jeder AST-Knoten enthält präzise Positionsinformationen (Zeile, Spalte und Zeichen-Offsets) aus dem Originaltext[dev.to](https://dev.to/wangpin34/how-to-retain-position-of-markdown-element-in-remarkjs-k8m#:~:text=,)[dev.to](https://dev.to/wangpin34/how-to-retain-position-of-markdown-element-in-remarkjs-k8m#:~:text=,11). Dadurch kann man z.B. Start- und End-Index eines Markdown-Elements im Rohtext bestimmen. Unerwünschte Knoten wie Codeblöcke (`<code>`) oder Blockquotes lassen sich anschließend einfach über ihren Typ aus dem Baum entfernen (Filtern), ohne die Positionsgenauigkeit der übrigen Knoten zu verlieren. Für Tabellen liefert der AST Parser sowohl den Tabelleninhalt als verschachtelte Knotenliste als auch die Header-Zeile als eigene Knoten.
* **CommonMark/Markdown-It** : Ähnlich wie remark in JS bieten CommonMark-basierte Parser (für verschiedene Sprachen, z.B. Python mit *markdown-it-py* oder Java mit  *flexmark* ) ebenfalls AST-Output mit optionalen Source-Positionen. Diese Parser können konfiguriert werden, bestimmte Elemente zu ignorieren. So können z.B. Codeblöcke und Zitate von vornherein übersprungen werden, falls der Parser diese Funktion unterstützt, oder man entfernt sie im Nachgang aus dem Strukturbaum.
* **Synonym-Handling bei Tabellen-Headern** : Da Tabellen in Requirements-Dokumenten oft unterschiedliche Spaltenbezeichnungen für dasselbe Konzept verwenden (z.B.  *Requirement* ,  *Requirement Text* , *Beschreibung* für die Anforderungsbeschreibung), empfiehlt es sich, nach dem AST-Parsing eine **Normalisierung** der Header vorzunehmen. Hierfür existieren 2025 noch keine dedizierten Standard-Bibliotheken, jedoch lassen sich mit einfachen Mappings oder Regex-Methoden Header-Aliasnamen auf Standardbegriffe vereinheitlichen. Beispielsweise kann man alle Varianten wie  *requirement* ,  *requirementText* , *Req. Description* auf einen einheitlichen Schlüssel *RequirementText* abbilden, bevor die Inhalte weiterverarbeitet werden.

**Best Practice:**

Zunächst wird das Markdown-Dokument vollständig geparst (z.B. mit *remark* oder einem äquivalenten Parser) um einen AST mit Positionsangaben zu erhalten. Anschließend filtert man aus dem AST alle **Codeblock-** und  **Blockquote** -Knoten heraus, sodass nur relevante Strukturen (Überschriften, Absätze, Listen, Tabellen etc.) verbleiben. Dann werden gezielt die gewünschten Inhalte extrahiert: Bei Fließtext etwa die Paragraphen oder Listenelemente, bei Tabellen die Zeilen. Tabellen-Header lassen sich dabei aus dem AST direkt ermitteln und mittels einer Synonym-Tabelle normalisiert (z.B. *“Id”* →  *“RequirementID”* , *“Requirement”* →  *“RequirementText”* ). Durch Verwendung der AST-Offets behält man für jede extrahierte Einheit ihren **Start/End-Index** im Originaltext, was für Traceability in späteren Schritten wichtig ist.

## 3. Segmentierung in atomare Strukturen inkl. Sampling (Window/Stride), `char_interval` und `section_path` Tracking

### Aktuelle Lösungen

* **Segment Any Text (SaT) / wtpsplit** : Dieses 2024 vorgestellte Verfahren gilt auch 2025 als führend für generische Textsegmentierung[aclanthology.org](https://aclanthology.org/2024.emnlp-main.665/#:~:text=adaptability%2C%20we%20introduce%20an%20extra,Our%20method%20outperforms%20all). Das Open-Source-Tool *wtpsplit* implementiert die SaT-Modelle und ermöglicht es, Texte in **Sätze oder semantische Einheiten** zu zerlegen[aclanthology.org](https://aclanthology.org/2024.emnlp-main.665/#:~:text=mixture%20of%20sentence,text%2Fwtpsplit%20under%20the%20MIT%20license). Im Gegensatz zu rein regelbasierten Ansätzen ist SaT robust gegenüber fehlender Interpunktion und in der Lage, Domänenwechsel zu bewältigen (z.B. Verse, rechtliche Texte)[aclanthology.org](https://aclanthology.org/2024.emnlp-main.665/#:~:text=adaptability%2C%20we%20introduce%20an%20extra,Our%20method%20outperforms%20all). Die Modelle arbeiten transformerbasiert und wurden hinsichtlich Geschwindigkeit optimiert (drei mal schneller als frühere Ansätze)[aclanthology.org](https://aclanthology.org/2024.emnlp-main.665/#:~:text=adaptability%2C%20we%20introduce%20an%20extra,Our%20method%20outperforms%20all). *wtpsplit* akzeptiert Fließtext beliebiger Länge und gibt die erkannten Segmentgrenzen aus – über diese lässt sich für jedes Segment direkt das **`char_interval`** (Start- und End-Index im Originaltext) berechnen. Hierdurch ist gewährleistet, dass jedes Segment genau auf den Ursprungs-Text gemappt ist.
* **Hierarchische Segmentierung & `section_path`:** Um **atomare Requirement-Einheiten** zu gewinnen, müssen oft hierarchische Strukturen berücksichtigt werden – z.B. einzelne Bullet-Points innerhalb verschachtelter Listen, oder Zeilen innerhalb von Tabellen, die jeweils eigenständige Anforderungen repräsentieren. Moderne Parser wie Docling stellen bereits beim Strukturparse einen **Section Path** für jeden Knoten bereit, der den Pfad im Dokumentbaum angibt (z.B. Kapitel 2 → Unterkapitel 2.1 → Liste → Listenpunkt 3)[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=)[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=Text%2C%20Tables%2C%20Pictures%2C%20Captions%2C%20Lists%2C,and%20more). In Kombination mit den Segmentierungs-Tools kann man so jedem Segment seinen umgebenden Abschnitt zuordnen. Praktisch geht man so vor: Erst werden anhand der Struktur (AST oder DoclingDocument) alle Textblöcke identifiziert (z.B. jeder Listenpunkt, jede Tabellenzelle mit Anforderungstext, jeder freie Absatz). Diese Blöcke können dann mit *wtpsplit* bei Bedarf weiter in Sätze zerlegt werden. Jedem resultierenden Segment wird der `section_path` seines Ursprungsblocks mitgegeben, sodass man weiß, in welchem Kontext (z.B. in welcher Tabelle oder Liste) es stand.
* **Windowed Sampling für sehr lange Dokumente:** Bei extrem großen Dokumenten (z.B. hunderte Seiten) empfiehlt es sich, zusätzlich mit **Fenstern und Überlappung (window/stride)** zu arbeiten, um Segmente in *Chuncks* aufzuteilen, die für nachfolgende LLM-Verarbeitung geeignet sind. Eine gängige Strategie 2025 ist, Segmente in sinnvollen Fenstern (z.B. 500 oder 1000 Tokens) mit einer Überlappung von 10–20% zu gruppieren. Dadurch geht bei der Verarbeitung durch ein Language Model der Zusammenhang an den Rändern nicht verloren. Frameworks wie *LangChain* oder *LlamaIndex* bieten hierfür Out-of-the-box Unterstützung – etwa mittels konfigurierbarer Chunker. Auch **Docling** liefert einen integrierten  *Chunker* : Die `DoclingDocument`-Klasse kann in einen Strom von Teilstücken („chunks“) zerlegt werden, jeweils mit zugehörigen Metadaten[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=serialization%20to%20,retain%20all%20available%20meta%20information)[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=A%20DoclingDocument%20can%20additionally%20be,provide%20a%20high%20degree%20of). Entwickler können eingebaute oder eigene Chunker-Strategien einsetzen, um große Texte gleitend zu segmentieren.

**Best Practice:**

Für die Gewinnung der atomaren Requirement-Statements wird zunächst anhand der Dokumentstruktur in sinnvolle Blöcke unterteilt (Absätze, Listenelemente, Tabelleneinträge). Diese Blöcke werden dann mit einem **Segmentierungsmodell** (z.B. wtpsplit) in Sätze oder Teilsätze zerlegt, falls nötig. Jeder Segment-Einheit werden zwei Attribute mitgegeben: das **`char_interval`** (Start- und Endposition im Originaldokument) und der **`section_path`** (z.B. welche Überschrift oder Liste übergeordnet ist). So bleibt der Bezug zur Ursprungsstruktur jederzeit erhalten. Bei sehr langen Dokumenten oder für die Verarbeitung mit LLMs werden die Segmente anschließend in überlappende Fenster gruppiert (z.B. je 1000 Token mit 200 Token Überlappung), um den Kontextfluss zu erhalten.

## 4. Validierung und Normalisierung von Offsets (SequenceMatcher, Whitespace-Kompression)

### Methoden & Frameworks

* **SequenceMatcher (Offset Alignment):** Kleine Abweichungen in den Offsets können auftreten, etwa durch Normalisierung von Zeilenenden oder Entfernen von Markdown-Syntax. Ein bewährter Ansatz ist der Einsatz eines Sequence Matchers (z.B. basierend auf dem Ratcliff/Obershelp-Algorithmus), um extrahierte Segmente mit dem Originaltext erneut abzugleichen. 2025 wird dies häufig als *Off-by-N-Korrekturschritt* in Toolkits integriert. Sollte z.B. ein Segment laut AST bei Zeichen 100 beginnen, aber aufgrund unsichtbarer Whitespaces tatsächlich erst bei 102, erkennt der SequenceMatcher das Muster und kann das `char_interval` entsprechend justieren. Viele Markdown-Parser-Toolchains nutzen hierfür Python’s `difflib.SequenceMatcher` oder ähnliche Implementierungen, um Original- und gefilterten Text zu vergleichen und systematische Verschiebungen zu korrigieren.
* **Whitespace Compress & Normalisierung:** Uneinheitliche Leerraumzeichen (Tabs vs. Spaces, multiple Leerzeilen etc.) können ebenfalls die Offset-Nachverfolgung erschweren. Als Best Practice wird empfohlen, **vor** dem Parsing eine Normalisierung durchzuführen – z.B. alle Tabulatoren in vier Spaces zu wandeln, überflüssige Leerzeilen zu reduzieren – und diese Normalisierung in die Offset-Berechnung einzubeziehen. Tools wie Docling führen intern bereits gewisse Normalisierungen durch (z.B. beim PDF-Parsing werden Spacing und Zeilenumbrüche bereinigt[dev.to](https://dev.to/novitzmann/docwire-sdk-20250619-released-major-ocr-pdf-layout-upgrades-archive-refactor-ci-improvements-k85#:~:text=%2A%20Plain,in%20PDF%20and%20OCR%20outputs)). Wichtig ist, diese Schritte zu protokollieren, um spätere Versatzkorrekturen nachzuvollziehen.
* **Automatisierte Offset-Validierung:** Einige Pipelines bieten automatisierte Prüfausgaben: *Surya* z.B. kann die erkannten Textbereiche visuell im Original-PDF hervorheben, was indirekt eine Kontrolle der Offsets darstellt. Docling stellt durch die *lossless JSON* Ausgabe sicher, dass jedes Element mit seinem Original-Index abgelegt wird[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=Besides%20specifying%20the%20data%20model%2C,retain%20all%20available%20meta%20information). Nach Zusammenführung der Segmente lohnt es sich, stichprobenartig zu validieren, ob die gespeicherten Offsets tatsächlich den entsprechenden Text im Quell-Dokument hervorheben – so lassen sich Fehler früh entdecken.

**Best Practice:**

Nach der initialen Segmentierung wird ein **Offset-Alignment** durchgeführt. Hierbei werden z.B. alle extrahierten Segment-Texte nochmals mit dem Originaltext abgeglichen. Differenzen – etwa durch entfernte Formatierungszeichen – kann man via SequenceMatcher erkennen und die gespeicherten `start`/`end` Indizes entsprechend korrigieren. Anschließend sollte man alle Offsets auf **Normiertheit** prüfen: Sind sie auf die Original-Zeichenzählung bezogen? Wurden z.B. Windows-Zeilenumbrüche konsistent berücksichtigt? Durch einen  *Whitespace-Compress* -Schritt vor dem Parsing (Konvertierung aller Steuerzeichen in eine einheitliche Darstellung) kann man viele Verschiebungen vermeiden. Insgesamt gilt: Lieber die Offsets  **einmal gründlich normalisieren und validieren** , als spätere Schritte mit schleichenden Fehlzuordnungen zu riskieren.

## 5. Kontextbewusste Verknüpfung benachbarter Komponenten (“prev/next”-Navigation)

### Fortschritte und Best Practices 2025

* **Order Preservation in Datenmodellen:** Moderne Parsing-Frameworks speichern Dokumentinhalte nicht nur hierarchisch, sondern auch in ihrer natürlichen Reihenfolge. Sowohl ein Markdown-AST als auch die Docling-Datenstruktur erhalten die  **Lesereihenfolge** : z.B. aufeinanderfolgende Absätze oder Listenelemente sind im Strukturmodell als aufeinander folgend repräsentiert. DocWire’s PDF-Parser sortiert Elemente explizit nach ihrer Position auf der Seite, um den Textfluss originalgetreu wiederzugeben[dev.to](https://dev.to/novitzmann/docwire-sdk-20250619-released-major-ocr-pdf-layout-upgrades-archive-refactor-ci-improvements-k85#:~:text=2%20%C2%B7%20Higher). Dank solcher Mechanismen lässt sich für jede atomare Anforderungs-Einheit ihr Vorgänger- und Nachfolger-Element bestimmen. Praktisch bedeutet dies: Hat man alle Segmente extrahiert und ihre `section_path`, so kann man anhand der ursprünglichen Reihenfolge im Dokument jedem Segment ein Feld `prev_component` und `next_component` zuweisen (bzw. IDs davon), außer natürlich am Anfang/Ende einer Sektion.
* **Graph-basierte Kontextverknüpfung:** Ein Trend 2025 ist die Nutzung von Graphstrukturen, um Dokumentkontext zu modellieren. Das  **LightRAG** -Framework z.B. erweitert Retrieval-Augmented Generation um Wissensgraphen und verknüpft Textelemente über Relationen[arxiv.org](https://arxiv.org/abs/2410.05779#:~:text=representations%20and%20inadequate%20contextual%20awareness%2C,This%20capability%20is%20further). Durch die Integration von Graph-Strukturen ins Indexing können Zusammenhänge und **Interdependencies** besser erfasst werden[arxiv.org](https://arxiv.org/abs/2410.05779#:~:text=representations%20and%20inadequate%20contextual%20awareness%2C,This%20capability%20is%20further). Übertragen auf Requirements bedeutet dies: Man kann ein **Kontext-Graph** aufbauen, in dem Knoten die einzelnen Requirement-Segmente sind und Kanten die “kommt nach”-Beziehung (und optional “gehört zu Abschnitt X” usw.) darstellen. Bei Anfragen oder Analysen können dann gezielt Nachbar-Notizen mit einbezogen werden. LightRAG demonstriert, dass solche Graph-gestützten Abrufe die Kontextrelevanz deutlich erhöhen[arxiv.org](https://arxiv.org/abs/2410.05779#:~:text=these%20challenges%2C%20we%20propose%20LightRAG%2C,This%20capability%20is%20further) – Antworten berücksichtigen also benachbarte Sätze besser.
* **LLM-gestützte Extraktion mit Kontext (ContextGem):** Das 2025 erschienene  *ContextGem* -Framework geht einen anderen Weg: es nutzt die großen Kontextfenster moderner LLMs, um **Dokumente im Ganzen oder abschnittsweise auszuwerten** und strukturierte Datenpunkte zu extrahieren[contextgem.dev](https://contextgem.dev/index.html#:~:text=ContextGem%20is%20a%20free%2C%20open,documents%20%E2%80%94%20with%20minimal%20code). Anstatt strikt benachbarte Segmente algorithmisch zu verlinken, speist ContextGem gleich ganze Dokumentpassagen oder das komplette Dokument in das LLM ein. Dieses kann dann in einem Durchgang z.B. alle Anforderungen mit ihren Zusammenhängen erkennen. ContextGem erleichtert diese Extraktion, indem es die Dokumente und zu suchenden *Aspekte* definiert (z.B. “Anforderungsbeschreibung”, “Akzeptanzkriterium”)[contextgem.dev](https://contextgem.dev/index.html#:~:text=Extracting%20Aspects)[contextgem.dev](https://contextgem.dev/index.html#:~:text=Learn%20how%20to%20extract%20and,documents%20using%20ContextGem%E2%80%99s%20Concepts%20API) und das LLM anleitet, die entsprechenden Inhalte auszulesen. Die **Prev/Next-Logik** wird hier implizit vom LLM verstanden, da es den umgebenden Text kennt. Für unsere Pipeline bedeutet das: Insbesondere bei der Evaluierung (Prüfung von Anforderungen im Kontext) können LLMs genutzt werden, um z.B. Dopplungen oder Widersprüche zu erkennen, wobei man ihnen stets benachbarte Segmente mit übergibt.

**Best Practice:**

Für eine deterministische Verarbeitung sollte jedes Segment Daten zu seinen Nachbarn tragen. Dies kann simpel über Indexierung gelöst werden (z.B. eine sortierte Liste aller Segmente pro `section_path` erzeugen und dann für jeden Eintrag die IDs von Vorgänger und Nachfolger speichern). Zusätzlich lohnt sich das Erstellen eines  **Kontextgraphen** : Knoten = Segmente, Kanten = *folgt auf* bzw.  *gehört zu* . Diese Graphstruktur kann man entweder direkt für Abfragen nutzen oder als Zusatzinformation in ein Language Model geben. In kritischen Anwendungen (z.B. automatische Konsistenzprüfungen) empfiehlt es sich, sowohl regelbasierte Kontexteinbindung (Prev/Next-IDs) als auch LLM-gestützte  *Windowed Context* -Analysen zu kombinieren, um vollständige Kontextabdeckung zu gewährleisten.

## 6. Fortschrittliche Deduplikationstechniken für komplex strukturierte und unstrukturierte Dokumente

### Frameworks & Methoden

* **MinHash LSH (Milvus 2.6):** Für großvolumige Textmengen – etwa in Requirements-Datenbanken oder vielen Dokumenten – hat sich *Locality-Sensitive Hashing* als performant erwiesen. Das Vektor-Datenbanksystem **Milvus** führte 2025 einen nativen MINHASH_LSH-Index ein, der speziell für Duplikaterkennung optimiert ist[milvus.io](https://milvus.io/blog/minhash-lsh-in-milvus-the-secret-weapon-for-fighting-duplicates-in-llm-training-data.md#:~:text=Core%20capabilities%20in%20Milvus%202,include). Über MinHash-Signaturen werden textuelle Inhalte so abgebildet, dass ähnliche/identische Texte mit hoher Wahrscheinlichkeit den gleichen Hash teilen[milvus.io](https://milvus.io/blog/minhash-lsh-in-milvus-the-secret-weapon-for-fighting-duplicates-in-llm-training-data.md#:~:text=Core%20capabilities%20in%20Milvus%202,include). Eine Jaccard-Ähnlichkeitssuche findet dann Near-Duplicates extrem schnell. Milvus 2.6 ermöglicht so die Skalierung auf Millionen von Segmenten und identifiziert doppelte oder sehr ähnliche Requirements (z.B. sich wiederholende Anforderungen in verschiedenen Doks) effizient[milvus.io](https://milvus.io/blog/minhash-lsh-in-milvus-the-secret-weapon-for-fighting-duplicates-in-llm-training-data.md#:~:text=A%20real,the%20leading%20LLM%20companies%E2%80%94approached%20us)[milvus.io](https://milvus.io/blog/minhash-lsh-in-milvus-the-secret-weapon-for-fighting-duplicates-in-llm-training-data.md#:~:text=match%20at%20L480%20MinHash%20LSH,management%2C%20plagiarism%20detection%2C%20and%20more). Diese Technik arbeitet inhaltsbasiert und ist unabhängig von der Dokumentstruktur.
* **Semantische Embeddings & Clustering:** Über **Embeddings** (numerische Vektorrepräsentationen von Text) lassen sich semantisch ähnliche Anforderungen erkennen, selbst wenn sie unterschiedlich formuliert sind. 2025 gibt es zahlreiche frei verfügbare Modelle (z.B.  *multilingual-e5-small* , BGE-Modelle), die Requirements-Sätze in hochdimensionale Vektoren einbetten. Ein Beispiel ist das  *DocWire SDK* , das im August 2025 ein Update erhielt, um lokal Embeddings zu erzeugen und darauf Cosine-Similarity-Berechnungen durchzuführen[sourceforge.net](https://sourceforge.net/projects/docwire/files/2025.08.05/#:~:text=This%20release%20introduces%20a%20major,particularly%20for%20MSVC%20and%20Valgrind)[sourceforge.net](https://sourceforge.net/projects/docwire/files/2025.08.05/#:~:text=,class%2C%20now%20powered%20by). Damit können nun offline **Clustering** und semantische Suchen über Dokumenteninhalte durchgeführt werden[sourceforge.net](https://sourceforge.net/projects/docwire/files/2025.08.05/#:~:text=,class%2C%20now%20powered%20by). Für Deduplikation bedeutet das: Man bettet alle Segmenttexte ein, führt einen Clustering-Algorithmus oder Ähnlichkeitssuche aus und findet Gruppen von Segmenten mit hoher inhaltlicher Übereinstimmung. Frameworks in Forschung und Praxis (z.B.  *PassionNet* ) kombinieren diese Embedding-Ähnlichkeiten mit regelbasierten Filtern[dblp.org](https://dblp.org/pid/240/1244#:~:text=Summra%20Saleem%20,i4). *PassionNet* (eine 2025 vorgestellte Methode) etwa entwirft Pipeline-Kombinationen, um doppelte und widersprüchliche Anforderungen automatisch zu erkennen[dblp.org](https://dblp.org/pid/240/1244#:~:text=Summra%20Saleem%20,i4). Hierbei werden sowohl klassische Features als auch neuronale Embeddings herangezogen, um auch Paraphrasen als Duplikate zu identifizieren.
* **Struktur- und Kontext-Constraints:** Neben rein inhaltlicher Ähnlichkeit können auch strukturelle Merkmale einbezogen werden, um Dubletten zu erkennen. Beispielsweise könnte man festlegen: Zwei Anforderungen, die im gleichen Dokument unter derselben Überschrift stehen und einen sehr ähnlichen Text haben, sind höchstwahrscheinlich Duplikate (ungewollte Wiederholung). Solche Regeln lassen sich in die Deduplizierungslogik integrieren, etwa indem man die `section_path`-Information mit auswertet. Einige kommerzielle Tools bieten Domänen-spezifische Deduplikation – z.B. **DocWire** erlaubt es, bei der Extraktion aus vielen Spezifikations-Dokumenten sofort Duplikate zu markieren, damit diese im Knowledge-Graph nur einmal angelegt werden (hier fließen oft Heuristiken ein, die angeben, welche Feldinhalte zur Duplikatsprüfung herangezogen werden).

**Best Practice:**

Die Deduplikation erfolgt idealerweise  **zweistufig** : (1) **Grobe Duplikatsuche** über effiziente Verfahren wie MinHash LSH – dies filtert potenzielle Dubletten mit hoher Recall-Rate. (2) **Feinabgleich** der Kandidaten mit semantischen Embeddings und Geschäftslogik. Im Feinabgleich bestimmt man einen Ähnlichkeitsschwellenwert (z.B. Cosine Similarity > 0.9) und prüft zusätzlich Kontext: liegen beide Segmente im selben Dokument/Kapitel? Sind beide als Requirement-Text klassifiziert? So können auch technisch nicht identische, aber inhaltlich redundante Anforderungen erkannt werden. Die zusammengeführten Duplikate sollten dokumentiert werden (z.B. in einem Merge-Report), damit bei Bedarf manuell verifiziert werden kann, ob es sich tatsächlich um eine unerwünschte Dopplung handelt. Moderne Tools wie DocWire bieten hierfür bereits Hilfsfunktionen – etwa Vektorindizes für Embeddings[sourceforge.net](https://sourceforge.net/projects/docwire/files/2025.08.05/#:~:text=This%20release%20introduces%20a%20major,particularly%20for%20MSVC%20and%20Valgrind) und direkte Similarity-Berechnungen[sourceforge.net](https://sourceforge.net/projects/docwire/files/2025.08.05/#:~:text=,class%2C%20now%20powered%20by), die in eigene Dedup-Pipelines integriert werden können.

## 7. Generierung strukturierter Outputs (Hierarchien & Relationen)

### Frameworks und Vorgehen

* **Docling JSON Export:** Docling’s vereinheitlichtes Datenmodell (DoclingDocument) kann **verlustfrei als JSON** serialisiert werden[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=Besides%20specifying%20the%20data%20model%2C,retain%20all%20available%20meta%20information). Das bedeutet: Alle erkannten Strukturelemente (Überschriften, Absätze, Listen, Tabellen, Bilder etc.) samt Hierarchie (Eltern-Kind-Beziehungen) und Positionen werden in einer JSON-Struktur festgehalten. Dieses JSON kann wiederum als Eingabe für nachgelagerte Prozesse oder zum Aufbau eines Knowledge Graph dienen. Im JSON sind beispielsweise Abschnittsknoten enthalten, die eine Liste ihrer Kinder (Paragraphs, Lists etc.) besitzen, und jedes Element trägt Metadaten wie Seitenzahl, Bounding Box (falls vom PDF) oder Text-Offsets. Damit lässt sich die gesamte Dokumentstruktur maschinenlesbar weiterverarbeiten.
* **Relationale Ausgabeformate (JSON-LD, Graph):** Über eine Anreicherung des JSON lassen sich auch Quer-Relationen ausdrücken. Beispielsweise könnte man in einem JSON-LD oder RDF-basierten Format Kanten für  *`prev`* , *`next`* oder *`duplicates`* zwischen Requirement-Knoten einfügen. Zwar bieten das die Parsing-Frameworks nicht von Haus aus, doch die zuvor erzeugten Informationen (siehe Punkte 5 und 6) kann man nutzen, um ein solches Format zu erzeugen. **DocWire** etwa verfügt über ein SDK, mit dem man extrahierte Inhalte direkt in Graph-Strukturen überführen kann (inklusive Verlinkung ähnlicher Inhalte). So könnte man aus mehreren Dokumenten einen Wissensgraphen bauen, in dem jeder Anforderungsknoten Attribute wie  *Text* ,  *Dokument* , *Kapitel* hat und Kanten zu *Duplikat von* oder *Folgt auf* anderen Knoten.
* **Beispielhafter Output:** Angenommen, man möchte einen hierarchischen JSON-Report einer Markdown-Spezifikation: Dieser könnte z.B. folgendermaßen aussehen (stark vereinfacht):

<pre class="overflow-visible!" data-start="25344" data-end="26167"><div class="contain-inline-size rounded-2xl relative bg-token-sidebar-surface-primary"><div class="sticky top-9"><div class="absolute end-0 bottom-0 flex h-9 items-center pe-2"><div class="bg-token-bg-elevated-secondary text-token-text-secondary flex items-center gap-4 rounded-sm px-2 font-sans text-xs"></div></div></div><div class="overflow-y-auto p-4" dir="ltr"><code class="whitespace-pre! language-json"><span><span>{</span><span>
  </span><span>"Document"</span><span>:</span><span></span><span>"Spec X"</span><span>,</span><span>
  </span><span>"Sections"</span><span>:</span><span></span><span>[</span><span>
    </span><span>{</span><span>
      </span><span>"title"</span><span>:</span><span></span><span>"1. Introduction"</span><span>,</span><span>
      </span><span>"children"</span><span>:</span><span></span><span>[</span><span>
        </span><span>{</span><span>
          </span><span>"type"</span><span>:</span><span></span><span>"paragraph"</span><span>,</span><span>
          </span><span>"text"</span><span>:</span><span></span><span>"This document describes..."</span><span>,</span><span>
          </span><span>"char_start"</span><span>:</span><span></span><span>0</span><span>,</span><span>
          </span><span>"char_end"</span><span>:</span><span></span><span>45</span><span>
        </span><span>}</span><span>
      </span><span>]</span><span>
    </span><span>}</span><span>,</span><span>
    </span><span>{</span><span>
      </span><span>"title"</span><span>:</span><span></span><span>"2. Requirements"</span><span>,</span><span>
      </span><span>"children"</span><span>:</span><span></span><span>[</span><span>
        </span><span>{</span><span>
          </span><span>"type"</span><span>:</span><span></span><span>"list_item"</span><span>,</span><span>
          </span><span>"id"</span><span>:</span><span></span><span>"REQ-1"</span><span>,</span><span>
          </span><span>"text"</span><span>:</span><span></span><span>"The system shall ..."</span><span>,</span><span>
          </span><span>"char_start"</span><span>:</span><span></span><span>100</span><span>,</span><span>
          </span><span>"char_end"</span><span>:</span><span></span><span>130</span><span>,</span><span>
          </span><span>"next"</span><span>:</span><span></span><span>"REQ-2"</span><span>,</span><span>
          </span><span>"duplicates"</span><span>:</span><span></span><span>[</span><span>]</span><span>
        </span><span>}</span><span>,</span><span>
        </span><span>{</span><span>
          </span><span>"type"</span><span>:</span><span></span><span>"list_item"</span><span>,</span><span>
          </span><span>"id"</span><span>:</span><span></span><span>"REQ-2"</span><span>,</span><span>
          </span><span>"text"</span><span>:</span><span></span><span>"The system shall ..."</span><span>,</span><span>
          </span><span>"char_start"</span><span>:</span><span></span><span>132</span><span>,</span><span>
          </span><span>"char_end"</span><span>:</span><span></span><span>160</span><span>,</span><span>
          </span><span>"prev"</span><span>:</span><span></span><span>"REQ-1"</span><span>,</span><span>
          </span><span>"duplicates"</span><span>:</span><span></span><span>[</span><span>"REQ-5"</span><span>]</span><span>
        </span><span>}</span><span>
      </span><span>]</span><span>
    </span><span>}</span><span>
  </span><span>]</span><span>
</span><span>}</span><span>
</span></span></code></div></div></pre>

Hier sieht man: Hierarchie (Section 2 enthält zwei Anforderungen), Offsets, sowie Relationen *prev/next* und ein Duplikats-Hinweis (*REQ-2* ist Duplikat von *REQ-5* irgendwo anders). Solche Outputs kann man als Entwickler frei gestalten. Wichtig ist, dass das Format einheitlich und gut dokumentiert ist, sodass die weiterverarbeitende Stelle (sei es ein Evaluationsskript oder ein Import in Jira o.ä.) die Informationen korrekt interpretieren kann.

* **Integration in Pipelines:** Die strukturierten Ausgaben werden typischerweise entweder als Datei exportiert (z.B. JSON/JSONL pro Dokument) oder direkt in Datenbanken/Indices eingespeist. 2025 ist es üblich, Anforderungen in **Vector-Datenbanken** (für semantische Suchen) und **Graph-Datenbanken** (für relationale Abfragen) abzulegen. Dank der umfassenden Vorverarbeitung lassen sich dabei reiche Attribute nutzen – z.B. können die Vektoren aus dem Embedding-Schritt (6) gleich mit gespeichert werden, ebenso die Graph-Kanten für Duplikate und Nachbarschaft.

**Best Practice:**

Setzen Sie auf ein **offenes, standardnahes Format** für den Output. JSON ist eine gute Basis, ggf. ergänzt um JSON-LD, um semantische Beziehungen auszudrücken. Nutzen Sie die Möglichkeiten der verwendeten Tools: Docling liefert bereits viel Struktur, die man direkt serialisieren kann[arxiv.org](https://arxiv.org/html/2501.17887v1#:~:text=Besides%20specifying%20the%20data%20model%2C,retain%20all%20available%20meta%20information). Sollte ein spezielles Zielformat nötig sein (z.B. ReqIF für Requirements oder ein Neo4j Graph), schreiben Sie eine Konvertierung, die das generische JSON in das Zielformat überführt. Achten Sie darauf, die Nachverfolgbarkeit (Traceability) sicherzustellen – d.h. jeder Output-Node sollte Referenzen (IDs oder Offsets) haben, um zurück ins Originaldokument zu gelangen. Durch die hier vorgestellten Verfahren ist es möglich, 2025 einen **durchgängigen automatisierten Requirements-Mining-Prozess** zu implementieren, der von heterogenen Quelldokumenten bis zum strukturierten Knowledge-Graph alles abbildet.
