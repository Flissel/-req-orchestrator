# Copy this file to ".env" and adjust the values.
# Example: cp .env.example .env

# ========= Service Ports =========
# Frontend (Vite dev server)
FRONTEND_PORT=3000
VITE_BACKEND_PORT=8087
VITE_ARCH_TEAM_PORT=8000

# Backend services
BACKEND_PORT=8087                # FastAPI main backend
ARCH_TEAM_PORT=8000             # Arch team agent service
AGENT_WORKER_PORT=8090          # Distributed agent worker

API_HOST=0.0.0.0

# ========= Database =========
SQLITE_PATH=/app/data/app.db
PURGE_RETENTION_H=24

# ========= LLM Provider: OpenRouter (Required) =========
# Sign up at https://openrouter.ai to get an API key
OPENROUTER_API_KEY=
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENAI_MODEL=google/gemini-2.5-flash:nitro

# Model Options (OpenRouter):
#   * google/gemini-2.5-flash:nitro (default) - Fast, cheap
#   * anthropic/claude-3-haiku - Fast, reliable
#   * anthropic/claude-3.5-sonnet - Best quality
#   * google/gemini-flash-1.5 - Fast alternative

# Mock Mode (uses heuristics instead of actual LLM calls)
MOCK_MODE=false

# LLM Fine-tuning
LLM_TEMPERATURE=0.0
LLM_TOP_P=1.0
LLM_MAX_TOKENS=0

# ========= Evaluation / Scoring =========
# VERDICT_THRESHOLD: Minimum score for a requirement to pass validation
# - 0.8: Strict (recommended for GPT-4)
# - 0.7: Balanced (recommended for Gemini Flash)
# - 0.6: Lenient
VERDICT_THRESHOLD=0.7
SUGGEST_MAX=10

# ========= Criteria / Prompts =========
CRITERIA_CONFIG_PATH=./config/criteria.json
EVAL_SYSTEM_PROMPT_PATH=./config/prompts/evaluate.system.txt
SUGGEST_SYSTEM_PROMPT_PATH=./config/prompts/suggest.system.txt
REWRITE_SYSTEM_PROMPT_PATH=./config/prompts/rewrite.system.txt

# ========= Qdrant / Vector Store =========
# Primary port: 6333 (standard Qdrant HTTP port)
# Fallback port: 6401 (docker-compose mapped port)
QDRANT_PORT=6333
QDRANT_PORT_FALLBACK=6401
QDRANT_URL=http://localhost
QDRANT_HOST=http://localhost
QDRANT_API_KEY=
QDRANT_COLLECTION=requirements_v1

# ========= Embeddings (OpenAI) =========
# Embeddings use OpenAI API (separate from LLM provider)
# Required for vector search / RAG functionality
OPENAI_API_KEY=
EMBEDDINGS_PROVIDER=openai
EMBEDDINGS_MODEL=text-embedding-3-small
EMBEDDINGS_DIM=1536

# ========= Batch / Parallelism =========
BATCH_SIZE=10
MAX_PARALLEL=5

# ========= I/O Paths =========
REQUIREMENTS_MD_PATH=./docs/requirements.md
OUTPUT_MD_PATH=./data/requirements.out.md

# ========= Logging =========
LOG_LEVEL=INFO

# ========= Concurrent Processing =========
VALIDATION_MAX_CONCURRENT=10
REWRITE_MAX_CONCURRENT=10
CLARIFICATION_MAX_CONCURRENT=10
DECISION_MAX_CONCURRENT=10
