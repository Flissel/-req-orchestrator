# Kopiere diese Datei nach ".env" und passe die Werte an.
# Beispiel: cp .env.example .env

# ========= Service Ports (Centralized Configuration) =========
# All services now use standardized port variables via backend/core/ports.py
# Legacy variables (API_PORT, APP_PORT, PORT) are deprecated but still supported

# Frontend (Vite dev server)
FRONTEND_PORT=3000
# For Vite proxy configuration (must be prefixed with VITE_ to be accessible in browser)
VITE_BACKEND_PORT=8087
VITE_ARCH_TEAM_PORT=8000

# Backend services
BACKEND_PORT=8087                # FastAPI main backend (replaces API_PORT)
ARCH_TEAM_PORT=8000             # Arch team Flask service (replaces APP_PORT)
AGENT_WORKER_PORT=8090          # Distributed agent worker (replaces PORT)

# Legacy variables (deprecated - use standardized names above)
API_HOST=0.0.0.0
# API_PORT=8087                 # DEPRECATED: Use BACKEND_PORT instead

# ========= Datenbank =========
SQLITE_PATH=/app/data/app.db
PURGE_RETENTION_H=24

# ========= LLM Provider: OpenRouter (Required) =========
# OpenRouter is the only supported LLM provider
# Sign up at https://openrouter.ai to get an API key

OPENROUTER_API_KEY=
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENAI_MODEL=google/gemini-2.5-flash:nitro

# Model Options (OpenRouter):
#   * google/gemini-2.5-flash:nitro (default) - Fast, cheap
#   * anthropic/claude-3-haiku - Fast, reliable
#   * anthropic/claude-3.5-sonnet - Best quality
#   * google/gemini-flash-1.5 - Fast alternative

# Mock Mode (uses heuristics instead of actual LLM calls)
MOCK_MODE=false

# LLM Fine-tuning
LLM_TEMPERATURE=0.0
LLM_TOP_P=1.0
LLM_MAX_TOKENS=0

# ========= Evaluation / Scoring =========
# VERDICT_THRESHOLD: Minimum score for a requirement to pass validation
# - 0.8: Strict (recommended for GPT-4)
# - 0.7: Balanced (recommended for Gemini Flash)
# - 0.6: Lenient
VERDICT_THRESHOLD=0.7
SUGGEST_MAX=10

# ========= Kriterien / Prompts =========
CRITERIA_CONFIG_PATH=./config/criteria.json
EVAL_SYSTEM_PROMPT_PATH=./config/prompts/evaluate.system.txt
SUGGEST_SYSTEM_PROMPT_PATH=./config/prompts/suggest.system.txt
REWRITE_SYSTEM_PROMPT_PATH=./config/prompts/rewrite.system.txt

# ========= Qdrant / Vector =========
# Primary port: 6333 (standard Qdrant HTTP port)
# Fallback port: 6401 (docker-compose mapped port)
# The system will try primary port first, then fallback if connection fails

QDRANT_PORT=6333                    # Primary Qdrant HTTP port
QDRANT_PORT_FALLBACK=6401           # Fallback port (used by docker-compose)
QDRANT_URL=http://localhost         # Base URL (port will be appended)
QDRANT_HOST=http://localhost        # Alternative name for base URL
QDRANT_API_KEY=
QDRANT_COLLECTION=requirements_v1

# Environment-specific overrides (automatically detected)
# CONFIG_ENV=dev                    # dev | docker-compose | production
# For docker-compose: QDRANT_URL=http://host.docker.internal
# For production: QDRANT_FULL_URL=https://qdrant.example.com:6333

# ========= Embeddings (OpenAI - separate from LLM provider) =========
# Embeddings still use OpenAI API (not OpenRouter)
# Required for vector search / RAG functionality
OPENAI_API_KEY=
EMBEDDINGS_PROVIDER=openai
EMBEDDINGS_MODEL=text-embedding-3-small
# DIM kann via Autodetect bestimmt werden; ansonsten Default setzen
EMBEDDINGS_DIM=1536

# ========= Batch / Parallelität =========
BATCH_SIZE=10
MAX_PARALLEL=5

# ========= IO Pfade =========
REQUIREMENTS_MD_PATH=./docs/requirements.md
OUTPUT_MD_PATH=./data/requirements.out.md

# ========= Versionierung / Canary =========
# FEATURE_FLAG_USE_V2: steuert Routing (v1 Legacy vs v2 FastAPI). CANARY_PERCENT in %.
FEATURE_FLAG_USE_V2=true
CANARY_PERCENT=100

# Legacy/V2 Basis-URLs (für Proxy/Frontend/Agenten)
LEGACY_BASE_URL=http://localhost:5000
V2_BASE_URL=http://localhost:8087

# ========= Logging =========
LOG_LEVEL=INFO

# ==============================================================================
# CONCURRENT PROCESSING (Performance-Tuning)
# ==============================================================================
VALIDATION_MAX_CONCURRENT=10
REWRITE_MAX_CONCURRENT=10
CLARIFICATION_MAX_CONCURRENT=10
DECISION_MAX_CONCURRENT=10

# =============================================================================